{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 台北市餐廳爬蟲\n",
    "\n",
    "使用網格化策略爬取台北市所有餐廳數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Restaurant:\n",
    "    name: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    address: str\n",
    "    phone: str = \"\"\n",
    "    rating: float = 0.0\n",
    "    price_level: int = 0\n",
    "    types: List[str] = None\n",
    "    source: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.types is None:\n",
    "            self.types = []\n",
    "\n",
    "# 台北市邊界定義\n",
    "TAIPEI_BOUNDS = {\n",
    "    'north': 25.2,  # 北緯\n",
    "    'south': 24.9,  # 南緯\n",
    "    'east': 121.7,  # 東經\n",
    "    'west': 121.4   # 西經\n",
    "}\n",
    "\n",
    "# 網格大小 (約1km)\n",
    "GRID_SIZE = 0.01\n",
    "\n",
    "print(f\"台北市邊界: {TAIPEI_BOUNDS}\")\n",
    "print(f\"網格大小: {GRID_SIZE}° (約1km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid() -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"生成台北市網格\"\"\"\n",
    "    grids = []\n",
    "    \n",
    "    lat_start = TAIPEI_BOUNDS['south']\n",
    "    lat_end = TAIPEI_BOUNDS['north']\n",
    "    lng_start = TAIPEI_BOUNDS['west']\n",
    "    lng_end = TAIPEI_BOUNDS['east']\n",
    "    \n",
    "    lat = lat_start\n",
    "    while lat < lat_end:\n",
    "        lng = lng_start\n",
    "        while lng < lng_end:\n",
    "            grids.append((lat, lat + GRID_SIZE, lng, lng + GRID_SIZE))\n",
    "            lng += GRID_SIZE\n",
    "        lat += GRID_SIZE\n",
    "    \n",
    "    return grids\n",
    "\n",
    "grids = generate_grid()\n",
    "print(f\"總共生成 {len(grids)} 個網格\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GooglePlacesScraper:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://maps.googleapis.com/maps/api/place\"\n",
    "    \n",
    "    def search_restaurants(self, lat: float, lng: float, radius: int = 1000) -> List[Restaurant]:\n",
    "        \"\"\"搜尋指定位置的餐廳\"\"\"\n",
    "        restaurants = []\n",
    "        \n",
    "        # 搜尋餐廳\n",
    "        url = f\"{self.base_url}/nearbysearch/json\"\n",
    "        params = {\n",
    "            'location': f\"{lat},{lng}\",\n",
    "            'radius': radius,\n",
    "            'type': 'restaurant',\n",
    "            'key': self.api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data['status'] == 'OK':\n",
    "                for place in data['results']:\n",
    "                    restaurant = Restaurant(\n",
    "                        name=place.get('name', ''),\n",
    "                        lat=place['geometry']['location']['lat'],\n",
    "                        lng=place['geometry']['location']['lng'],\n",
    "                        address=place.get('vicinity', ''),\n",
    "                        rating=place.get('rating', 0.0),\n",
    "                        price_level=place.get('price_level', 0),\n",
    "                        types=place.get('types', []),\n",
    "                        source='google_places'\n",
    "                    )\n",
    "                    restaurants.append(restaurant)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Google Places API 錯誤: {e}\")\n",
    "        \n",
    "        return restaurants\n",
    "\n",
    "# 使用範例 (需要 API key)\n",
    "# scraper = GooglePlacesScraper('YOUR_API_KEY')\n",
    "# restaurants = scraper.search_restaurants(25.0330, 121.5654)  # 台北101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenStreetMapScraper:\n",
    "    \"\"\"使用 Overpass API 爬取 OpenStreetMap 數據\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    def search_restaurants(self, lat_min: float, lat_max: float, \n",
    "                          lng_min: float, lng_max: float) -> List[Restaurant]:\n",
    "        \"\"\"搜尋指定區域的餐廳\"\"\"\n",
    "        restaurants = []\n",
    "        \n",
    "        # Overpass QL 查詢\n",
    "        query = f\"\"\"\n",
    "[out:json][timeout:25];\n",
    "(\n",
    "  node[\"amenity\"=\"restaurant\"]({lat_min},{lng_min},{lat_max},{lng_max});\n",
    "  way[\"amenity\"=\"restaurant\"]({lat_min},{lng_min},{lat_max},{lng_max});\n",
    "  relation[\"amenity\"=\"restaurant\"]({lat_min},{lng_min},{lat_max},{lng_max});\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out skel qt;\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, params={'data': query})\n",
    "            data = response.json()\n",
    "            \n",
    "            for element in data.get('elements', []):\n",
    "                if element['type'] == 'node' and 'tags' in element:\n",
    "                    tags = element['tags']\n",
    "                    restaurant = Restaurant(\n",
    "                        name=tags.get('name', ''),\n",
    "                        lat=element['lat'],\n",
    "                        lng=element['lon'],\n",
    "                        address=tags.get('addr:street', ''),\n",
    "                        phone=tags.get('phone', ''),\n",
    "                        types=[tags.get('cuisine', '')] if tags.get('cuisine') else [],\n",
    "                        source='openstreetmap'\n",
    "                    )\n",
    "                    restaurants.append(restaurant)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenStreetMap API 錯誤: {e}\")\n",
    "        \n",
    "        return restaurants\n",
    "\n",
    "osm_scraper = OpenStreetMapScraper()\n",
    "print(\"OpenStreetMap 爬蟲已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaipeiOpenDataScraper:\n",
    "    \"\"\"台北市政府開放資料爬蟲\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://data.taipei/api/v1/dataset\"\n",
    "    \n",
    "    def get_restaurants(self) -> List[Restaurant]:\n",
    "        \"\"\"獲取台北市餐廳資料\"\"\"\n",
    "        restaurants = []\n",
    "        \n",
    "        # 台北市餐廳資料集 ID (需要查詢實際的資料集)\n",
    "        dataset_id = \"your_dataset_id\"\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/{dataset_id}?scope=resourceAquire\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            \n",
    "            for item in data.get('result', {}).get('results', []):\n",
    "                restaurant = Restaurant(\n",
    "                    name=item.get('name', ''),\n",
    "                    lat=float(item.get('lat', 0)),\n",
    "                    lng=float(item.get('lng', 0)),\n",
    "                    address=item.get('address', ''),\n",
    "                    phone=item.get('phone', ''),\n",
    "                    source='taipei_open_data'\n",
    "                )\n",
    "                restaurants.append(restaurant)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"台北開放資料 API 錯誤: {e}\")\n",
    "        \n",
    "        return restaurants\n",
    "\n",
    "print(\"台北開放資料爬蟲已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestaurantCrawler:\n",
    "    \"\"\"整合多個數據源的餐廳爬蟲\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.osm_scraper = OpenStreetMapScraper()\n",
    "        self.taipei_scraper = TaipeiOpenDataScraper()\n",
    "        self.all_restaurants = []\n",
    "    \n",
    "    def crawl_grid(self, grid: Tuple[float, float, float, float]) -> List[Restaurant]:\n",
    "        \"\"\"爬取單個網格的餐廳\"\"\"\n",
    "        lat_min, lat_max, lng_min, lng_max = grid\n",
    "        restaurants = []\n",
    "        \n",
    "        # 從 OpenStreetMap 爬取\n",
    "        osm_restaurants = self.osm_scraper.search_restaurants(lat_min, lat_max, lng_min, lng_max)\n",
    "        restaurants.extend(osm_restaurants)\n",
    "        \n",
    "        logger.info(f\"網格 ({lat_min:.4f}, {lng_min:.4f}) 找到 {len(osm_restaurants)} 家餐廳\")\n",
    "        \n",
    "        return restaurants\n",
    "    \n",
    "    def crawl_all_grids(self, max_workers: int = 5) -> List[Restaurant]:\n",
    "        \"\"\"並行爬取所有網格\"\"\"\n",
    "        all_restaurants = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # 提交所有網格任務\n",
    "            future_to_grid = {executor.submit(self.crawl_grid, grid): grid for grid in grids}\n",
    "            \n",
    "            # 收集結果\n",
    "            for future in as_completed(future_to_grid):\n",
    "                grid = future_to_grid[future]\n",
    "                try:\n",
    "                    restaurants = future.result()\n",
    "                    all_restaurants.extend(restaurants)\n",
    "                    \n",
    "                    # 避免 API 限制\n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"網格 {grid} 爬取失敗: {e}\")\n",
    "        \n",
    "        return all_restaurants\n",
    "    \n",
    "    def save_to_csv(self, restaurants: List[Restaurant], filename: str = \"taipei_restaurants.csv\"):\n",
    "        \"\"\"保存到 CSV\"\"\"\n",
    "        data = []\n",
    "        for restaurant in restaurants:\n",
    "            data.append({\n",
    "                'name': restaurant.name,\n",
    "                'lat': restaurant.lat,\n",
    "                'lng': restaurant.lng,\n",
    "                'address': restaurant.address,\n",
    "                'phone': restaurant.phone,\n",
    "                'rating': restaurant.rating,\n",
    "                'price_level': restaurant.price_level,\n",
    "                'types': ','.join(restaurant.types),\n",
    "                'source': restaurant.source\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"已保存 {len(restaurants)} 家餐廳到 {filename}\")\n",
    "    \n",
    "    def remove_duplicates(self, restaurants: List[Restaurant]) -> List[Restaurant]:\n",
    "        \"\"\"移除重複餐廳 (基於名稱和位置)\"\"\"\n",
    "        seen = set()\n",
    "        unique_restaurants = []\n",
    "        \n",
    "        for restaurant in restaurants:\n",
    "            # 使用名稱和位置作為唯一標識\n",
    "            key = (restaurant.name, round(restaurant.lat, 4), round(restaurant.lng, 4))\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_restaurants.append(restaurant)\n",
    "        \n",
    "        return unique_restaurants\n",
    "\n",
    "crawler = RestaurantCrawler()\n",
    "print(\"餐廳爬蟲已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始爬取 (小規模測試)\n",
    "print(\"開始爬取台北市餐廳...\")\n",
    "\n",
    "# 先測試前5個網格\n",
    "test_grids = grids[:5]\n",
    "test_restaurants = []\n",
    "\n",
    "for grid in test_grids:\n",
    "    restaurants = crawler.crawl_grid(grid)\n",
    "    test_restaurants.extend(restaurants)\n",
    "    time.sleep(1)  # 避免過於頻繁的請求\n",
    "\n",
    "print(f\"測試爬取完成，找到 {len(test_restaurants)} 家餐廳\")\n",
    "\n",
    "# 移除重複\n",
    "unique_restaurants = crawler.remove_duplicates(test_restaurants)\n",
    "print(f\"去重後剩餘 {len(unique_restaurants)} 家餐廳\")\n",
    "\n",
    "# 顯示前10家餐廳\n",
    "for i, restaurant in enumerate(unique_restaurants[:10]):\n",
    "    print(f\"{i+1}. {restaurant.name} - {restaurant.address} ({restaurant.lat:.4f}, {restaurant.lng:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存測試結果\n",
    "if 'unique_restaurants' in locals() and unique_restaurants:\n",
    "    crawler.save_to_csv(unique_restaurants, \"taipei_restaurants_test.csv\")\n",
    "    \n",
    "    # 顯示統計信息\n",
    "    df = pd.DataFrame([{\n",
    "        'name': r.name,\n",
    "        'lat': r.lat,\n",
    "        'lng': r.lng,\n",
    "        'address': r.address,\n",
    "        'source': r.source\n",
    "    } for r in unique_restaurants])\n",
    "    \n",
    "    print(\"\\n統計信息:\")\n",
    "    print(f\"總餐廳數: {len(df)}\")\n",
    "    print(f\"數據源分布:\\n{df['source'].value_counts()}\")\n",
    "    print(f\"\\n經緯度範圍:\")\n",
    "    print(f\"緯度: {df['lat'].min():.4f} - {df['lat'].max():.4f}\")\n",
    "    print(f\"經度: {df['lng'].min():.4f} - {df['lng'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整爬取 (謹慎使用，可能需要很長時間)\n",
    "def full_crawl():\n",
    "    \"\"\"完整爬取所有網格\"\"\"\n",
    "    print(\"開始完整爬取...\")\n",
    "    print(f\"總共 {len(grids)} 個網格\")\n",
    "    \n",
    "    all_restaurants = crawler.crawl_all_grids(max_workers=3)\n",
    "    \n",
    "    print(f\"爬取完成，總共找到 {len(all_restaurants)} 家餐廳\")\n",
    "    \n",
    "    # 移除重複\n",
    "    unique_restaurants = crawler.remove_duplicates(all_restaurants)\n",
    "    print(f\"去重後剩餘 {len(unique_restaurants)} 家餐廳\")\n",
    "    \n",
    "    # 保存結果\n",
    "    crawler.save_to_csv(unique_restaurants, \"taipei_restaurants_full.csv\")\n",
    "    \n",
    "    return unique_restaurants\n",
    "\n",
    "# 取消註釋來執行完整爬取\n",
    "# full_results = full_crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數據分析和可視化\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_restaurants(restaurants: List[Restaurant]):\n",
    "    \"\"\"分析餐廳數據\"\"\"\n",
    "    if not restaurants:\n",
    "        print(\"沒有餐廳數據可分析\")\n",
    "        return\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        'name': r.name,\n",
    "        'lat': r.lat,\n",
    "        'lng': r.lng,\n",
    "        'address': r.address,\n",
    "        'rating': r.rating,\n",
    "        'source': r.source\n",
    "    } for r in restaurants])\n",
    "    \n",
    "    # 繪製分布圖\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. 餐廳位置分布\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(df['lng'], df['lat'], alpha=0.6, s=10)\n",
    "    plt.title('台北市餐廳分布')\n",
    "    plt.xlabel('經度')\n",
    "    plt.ylabel('緯度')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. 數據源分布\n",
    "    plt.subplot(2, 2, 2)\n",
    "    df['source'].value_counts().plot(kind='bar')\n",
    "    plt.title('數據源分布')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. 評分分布\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if df['rating'].sum() > 0:\n",
    "        df['rating'].hist(bins=20)\n",
    "        plt.title('餐廳評分分布')\n",
    "        plt.xlabel('評分')\n",
    "        plt.ylabel('數量')\n",
    "    \n",
    "    # 4. 密度熱圖\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hexbin(df['lng'], df['lat'], gridsize=20, cmap='YlOrRd')\n",
    "    plt.title('餐廳密度熱圖')\n",
    "    plt.xlabel('經度')\n",
    "    plt.ylabel('緯度')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 統計信息\n",
    "    print(\"\\n=== 統計信息 ===\")\n",
    "    print(f\"總餐廳數: {len(df)}\")\n",
    "    print(f\"數據源分布:\\n{df['source'].value_counts()}\")\n",
    "    if df['rating'].sum() > 0:\n",
    "        print(f\"平均評分: {df['rating'].mean():.2f}\")\n",
    "        print(f\"最高評分: {df['rating'].max():.2f}\")\n",
    "    print(f\"經緯度範圍: 緯度({df['lat'].min():.4f}, {df['lat'].max():.4f}), 經度({df['lng'].min():.4f}, {df['lng'].max():.4f})\")\n",
    "\n",
    "# 分析測試數據\n",
    "if 'unique_restaurants' in locals() and unique_restaurants:\n",
    "    analyze_restaurants(unique_restaurants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
